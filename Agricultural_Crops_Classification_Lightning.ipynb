{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4122124,"sourceType":"datasetVersion","datasetId":2436336}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-05-05T11:17:22.369701Z","iopub.execute_input":"2024-05-05T11:17:22.370100Z","iopub.status.idle":"2024-05-05T11:17:42.946461Z","shell.execute_reply.started":"2024-05-05T11:17:22.370070Z","shell.execute_reply":"2024-05-05T11:17:42.945411Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.2.4-py3-none-any.whl.metadata (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.2)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2+cpu)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.2)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nDownloading lightning-2.2.4-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.2.4\n","output_type":"stream"}]},{"cell_type":"code","source":"# Basic libraries\nimport os\nimport numpy\nimport torch\nfrom torch import nn\nimport pandas\nimport matplotlib.pyplot as plt\n\n# PyTorch data processing libraries\nfrom torchvision import transforms, datasets\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# PyTorch Model Implementation Libraries\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torchmetrics import Accuracy\n\n# PyTorch Lightning\nfrom lightning import LightningDataModule, LightningModule\nimport lightning as L\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:33:45.502418Z","iopub.execute_input":"2024-05-05T11:33:45.502907Z","iopub.status.idle":"2024-05-05T11:33:45.514513Z","shell.execute_reply.started":"2024-05-05T11:33:45.502872Z","shell.execute_reply":"2024-05-05T11:33:45.513554Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Set up train and test paths\ndata_path = '/kaggle/input/agricultural-crops-image-classification/Agricultural-crops'","metadata":{"execution":{"iopub.status.busy":"2024-05-05T11:18:24.594843Z","iopub.execute_input":"2024-05-05T11:18:24.595872Z","iopub.status.idle":"2024-05-05T11:18:24.601515Z","shell.execute_reply.started":"2024-05-05T11:18:24.595827Z","shell.execute_reply":"2024-05-05T11:18:24.600213Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CropsDataModule(L.LightningDataModule):\n    def __init__(self, data_dir: str, batch_size: int = 32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        \n    def prepare_data(self):\n        # download the data (since we are using Kaggle, there's no need for that)\n        # When downloading the data, unzip it and check it here\n        pass\n    \n    def setup(self, stage):\n        # Define base transformations\n        base_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n        \n        # Enhanced transformations for training with data augmentation\n        train_transform = transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            base_transform\n        ])\n\n        # More consistent transformations for validation and testing\n        test_val_transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            base_transform\n        ])\n        \n        # Assign a train and valid dataset to use in dataloaders\n        dataset = datasets.ImageFolder(root=self.data_dir)\n        \n        train_size = int(len(dataset) * 0.7)\n        valid_size = int(len(dataset) * 0.15)\n        test_size = len(dataset) - train_size - valid_size\n        \n        # Random split with a seed for reproducibility\n        seed = torch.Generator().manual_seed(42)\n        self.train_set, self.valid_set, self.test_set = random_split(dataset,\n                                                                     [train_size, valid_size, test_size],\n                                                                     generator=seed)\n        # Apply the transformations\n        if stage == 'fit' or stage is None:\n            self.train_set.dataset.transform = train_transform\n            \n        if stage in {'validate', 'test', 'predict'} or stage is None:\n            self.valid_set.dataset.transform = test_val_transform\n            self.test_set.dataset.transform = test_val_transform\n            \n    def train_dataloader(self):\n        print(\"Creating training dataloader\")\n        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=3, shuffle=True)\n    \n    def val_dataloader(self):\n        print(\"Creating validation dataloader\")\n        return DataLoader(self.valid_set, batch_size=self.batch_size, num_workers=3)\n    \n    def test_dataloader(self):\n        print(\"Creating test dataloader\")\n        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=3)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:17:01.949826Z","iopub.execute_input":"2024-05-05T12:17:01.950465Z","iopub.status.idle":"2024-05-05T12:17:01.966480Z","shell.execute_reply.started":"2024-05-05T12:17:01.950428Z","shell.execute_reply":"2024-05-05T12:17:01.965508Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"dm = CropsDataModule(data_dir=data_path, batch_size=16)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = dm.train_dataloader()\nval_loader = dm.val_dataloader()\ntest_loader = dm.test_dataloader()\n\n# Function to check a few batches\ndef check_dataloader(dataloader, name):\n    print(f\"Checking {name} DataLoader:\")\n    for i, (data, target) in enumerate(dataloader):\n        print(f\"  Batch {i + 1}:\")\n        print(f\"    Data shape: {data.shape}, Type: {data.dtype}\")\n        print(f\"    Target shape: {target.shape}, Type: {target.dtype}\")\n        if i >= 2:  # Check only the first 3 batches\n            break\n\ncheck_dataloader(train_loader, \"Training\")\ncheck_dataloader(val_loader, \"Validation\")\ncheck_dataloader(test_loader, \"Testing\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:17:05.207681Z","iopub.execute_input":"2024-05-05T12:17:05.208384Z","iopub.status.idle":"2024-05-05T12:17:08.521861Z","shell.execute_reply.started":"2024-05-05T12:17:05.208347Z","shell.execute_reply":"2024-05-05T12:17:08.520622Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Creating training dataloader\nCreating validation dataloader\nCreating test dataloader\nChecking Training DataLoader:\n  Batch 1:\n    Data shape: torch.Size([32, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([32]), Type: torch.int64\n  Batch 2:\n    Data shape: torch.Size([32, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([32]), Type: torch.int64\n  Batch 3:\n    Data shape: torch.Size([32, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([32]), Type: torch.int64\nChecking Validation DataLoader:\n  Batch 1:\n    Data shape: torch.Size([32, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([32]), Type: torch.int64\n  Batch 2:\n    Data shape: torch.Size([32, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([32]), Type: torch.int64\n  Batch 3:\n    Data shape: torch.Size([32, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([32]), Type: torch.int64\nChecking Testing DataLoader:\n  Batch 1:\n    Data shape: torch.Size([32, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([32]), Type: torch.int64\n  Batch 2:\n    Data shape: torch.Size([32, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([32]), Type: torch.int64\n  Batch 3:\n    Data shape: torch.Size([32, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([32]), Type: torch.int64\n","output_type":"stream"}]},{"cell_type":"code","source":"class ImageNetTL(L.LightningModule):\n    def __init__(self, num_target_classes: int, learning_rate: float = 0.001):\n        super().__init__()\n        self.save_hyperparameters()  # This saves learning_rate and num_target_classes as part of model hyperparameters\n        self.validation_step_outputs = []\n        \n        backbone = models.resnet50(weights=\"DEFAULT\")\n        num_features = backbone.fc.in_features # Save the features of the last layer\n        layers = list(backbone.children())[:-1] # Remove the last layer\n        self.feature_extractor = nn.Sequential(*layers) # Wrap the remaining layers\n        \n        self.num_target_classes = num_target_classes\n        self.classifier = nn.Linear(num_features, num_target_classes)\n        \n        # Evaluation metrics\n        self.train_acc = Accuracy(task='multiclass', num_classes=num_target_classes)\n        self.valid_acc = Accuracy(task='multiclass', num_classes=num_target_classes)\n        self.test_acc = Accuracy(task='multiclass', num_classes=num_target_classes)\n        \n    def forward(self, x):\n        representations = self.feature_extractor(x).flatten(1)\n        return self.classifier(representations)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        loss = F.cross_entropy(preds, y)\n        self.train_acc(preds, y)\n        self.log('train_loss', loss)\n        self.log('train_acc', self.train_acc.compute().mean(), on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        val_loss = F.cross_entropy(preds, y)\n        self.valid_acc(preds, y)\n        self.validation_step_outputs.append(val_loss)\n        self.log('val_loss', val_loss, on_step=False, on_epoch=True)\n        return {'val_loss': val_loss}\n    \n    # Added to make use of all the outputs from each `validation_step`\n    def on_validation_epoch_end(self):\n        epoch_average = torch.stack(self.validation_step_outputs).mean()\n        self.log(\"validation_epoch_average\", epoch_average)\n        self.validation_step_outputs.clear()  # free memory\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        test_loss = F.cross_entropy(preds, y)\n        self.log(\"test_loss\", test_loss, on_step=False, on_epoch=True)\n        return {'test_loss': test_loss}\n    \n    def on_test_epoch_end(self, outputs):\n        self.log('test_acc', self.test_acc.compute(), on_epoch=True, prog_bar=True)\n        self.test_acc.reset()\n        \n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:19:56.143287Z","iopub.execute_input":"2024-05-05T12:19:56.143932Z","iopub.status.idle":"2024-05-05T12:19:56.162242Z","shell.execute_reply.started":"2024-05-05T12:19:56.143897Z","shell.execute_reply":"2024-05-05T12:19:56.161000Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model = ImageNetTL(num_target_classes=30)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:19:58.839523Z","iopub.execute_input":"2024-05-05T12:19:58.839937Z","iopub.status.idle":"2024-05-05T12:20:00.221321Z","shell.execute_reply.started":"2024-05-05T12:19:58.839906Z","shell.execute_reply":"2024-05-05T12:20:00.220192Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n\n  0%|          | 0.00/97.8M [00:00<?, ?B/s]\u001b[A\n  4%|▍         | 4.12M/97.8M [00:00<00:02, 43.2MB/s]\u001b[A\n 17%|█▋        | 16.7M/97.8M [00:00<00:00, 95.4MB/s]\u001b[A\n 30%|███       | 29.6M/97.8M [00:00<00:00, 114MB/s] \u001b[A\n 44%|████▍     | 42.9M/97.8M [00:00<00:00, 124MB/s]\u001b[A\n 58%|█████▊    | 57.2M/97.8M [00:00<00:00, 133MB/s]\u001b[A\n 73%|███████▎  | 71.5M/97.8M [00:00<00:00, 139MB/s]\u001b[A\n100%|██████████| 97.8M/97.8M [00:00<00:00, 130MB/s]\u001b[A\n","output_type":"stream"}]},{"cell_type":"code","source":"dm.setup('fit')\ntrainer = L.Trainer(max_epochs=1, log_every_n_steps=10)\n#print(next(iter(dm.train_dataloader())))  # Test the iterability directly here\ntrainer.fit(model, datamodule=dm)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T12:20:13.952350Z","iopub.execute_input":"2024-05-05T12:20:13.952780Z","iopub.status.idle":"2024-05-05T12:20:42.862787Z","shell.execute_reply.started":"2024-05-05T12:20:13.952747Z","shell.execute_reply":"2024-05-05T12:20:42.861441Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"INFO: GPU available: False, used: False\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nINFO: \n  | Name              | Type               | Params\n---------------------------------------------------------\n0 | feature_extractor | Sequential         | 23.5 M\n1 | classifier        | Linear             | 61.5 K\n2 | train_acc         | MulticlassAccuracy | 0     \n3 | valid_acc         | MulticlassAccuracy | 0     \n4 | test_acc          | MulticlassAccuracy | 0     \n---------------------------------------------------------\n23.6 M    Trainable params\n0         Non-trainable params\n23.6 M    Total params\n94.278    Total estimated model params size (MB)\n","output_type":"stream"},{"name":"stdout","text":"Training set size: 580\nValidation set size: 124\nTest set size: 125\nTraining set size: 580\nValidation set size: 124\nTest set size: 125\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Creating validation dataloader\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"name":"stdout","text":"Creating training dataloader\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6b892b3adb449d19b852dabbcb8d83c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize EarlyStopping callback to monitor 'val_loss' for a patience of 3 epochs\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.00,\n    patience=3,\n    verbose=False,\n    mode='min'\n)\n\ntrainer = L.Trainer(max_epochs=10,\n                    profiler='simple', log_every_n_steps=10)\ntrainer.fit(model, datamodule=dm)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T02:04:11.204201Z","iopub.execute_input":"2024-05-04T02:04:11.204921Z","iopub.status.idle":"2024-05-04T02:04:12.810749Z","shell.execute_reply.started":"2024-05-04T02:04:11.204887Z","shell.execute_reply":"2024-05-04T02:04:12.809169Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"DataLoaders are iterable and working as expected.\n","output_type":"stream"},{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name              | Type               | Params\n---------------------------------------------------------\n0 | feature_extractor | Sequential         | 23.5 M\n1 | classifier        | Linear             | 61.5 K\n2 | train_acc         | MulticlassAccuracy | 0     \n3 | valid_acc         | MulticlassAccuracy | 0     \n4 | test_acc          | MulticlassAccuracy | 0     \n---------------------------------------------------------\n23.6 M    Trainable params\n0         Non-trainable params\n23.6 M    Total params\n94.278    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba3c7c078cb482a8869ca0077107341"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:402\u001b[0m, in \u001b[0;36m_check_dataloader_iterable\u001b[0;34m(dataloader, source, trainer_fn)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;66;03m# A prefix in the message to disambiguate between the train- and (optional) val dataloader that .fit() accepts\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: 'CropsDataModule' object is not iterable","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 29\u001b[0m\n\u001b[1;32m     19\u001b[0m early_stop_callback \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m     20\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     28\u001b[0m                     profiler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1031\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1057\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:110\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;129m@_no_grad_context\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_OUT_DICT]:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:179\u001b[0m, in \u001b[0;36m_EvaluationLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dl \u001b[38;5;129;01min\u001b[39;00m combined_loader\u001b[38;5;241m.\u001b[39mflattened:\n\u001b[0;32m--> 179\u001b[0m     \u001b[43m_check_dataloader_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     dl \u001b[38;5;241m=\u001b[39m _process_dataloader(trainer, trainer_fn, stage, dl)\n\u001b[1;32m    181\u001b[0m     dataloaders\u001b[38;5;241m.\u001b[39mappend(dl)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:407\u001b[0m, in \u001b[0;36m_check_dataloader_iterable\u001b[0;34m(dataloader, source, trainer_fn)\u001b[0m\n\u001b[1;32m    405\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer_fn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source\u001b[38;5;241m.\u001b[39mis_module():\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn invalid dataloader was passed to `Trainer.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer_fn\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mdataloaders=...)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataloader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_overridden(source\u001b[38;5;241m.\u001b[39mname, source\u001b[38;5;241m.\u001b[39minstance):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    413\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn invalid dataloader was passed to `Trainer.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer_fn\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mdataloaders=...)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataloader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Either pass the dataloader to the `.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer_fn\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()` method OR implement\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `def \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(self):` in your LightningModule/LightningDataModule.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     )\n","\u001b[0;31mTypeError\u001b[0m: An invalid dataloader was passed to `Trainer.fit(train_dataloaders=...)`. Found <__main__.CropsDataModule object at 0x790c9c9f8b20>."],"ename":"TypeError","evalue":"An invalid dataloader was passed to `Trainer.fit(train_dataloaders=...)`. Found <__main__.CropsDataModule object at 0x790c9c9f8b20>.","output_type":"error"}]},{"cell_type":"code","source":"%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show pytorch-lightning\n!pip install --upgrade pytorch-lightning","metadata":{"execution":{"iopub.status.busy":"2024-05-04T01:56:04.053039Z","iopub.execute_input":"2024-05-04T01:56:04.053717Z","iopub.status.idle":"2024-05-04T01:56:34.569762Z","shell.execute_reply.started":"2024-05-04T01:56:04.053680Z","shell.execute_reply":"2024-05-04T01:56:34.568480Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Name: pytorch-lightning\nVersion: 2.2.2\nSummary: PyTorch Lightning is the lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.\nHome-page: https://github.com/Lightning-AI/lightning\nAuthor: Lightning AI et al.\nAuthor-email: pytorch@lightning.ai\nLicense: Apache-2.0\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: fsspec, lightning-utilities, numpy, packaging, PyYAML, torch, torchmetrics, tqdm, typing-extensions\nRequired-by: lightning\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (2.2.2)\nCollecting pytorch-lightning\n  Downloading pytorch_lightning-2.2.4-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.26.4)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (2.1.2)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.66.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (6.0.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (1.3.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.9.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.11.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch-lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\nDownloading pytorch_lightning-2.2.4-py3-none-any.whl (802 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.2/802.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pytorch-lightning\n  Attempting uninstall: pytorch-lightning\n    Found existing installation: pytorch-lightning 2.2.2\n    Uninstalling pytorch-lightning-2.2.2:\n      Successfully uninstalled pytorch-lightning-2.2.2\nSuccessfully installed pytorch-lightning-2.2.4\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
