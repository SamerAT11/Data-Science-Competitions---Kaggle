{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4122124,"sourceType":"datasetVersion","datasetId":2436336}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-05-06T14:49:49.773429Z","iopub.execute_input":"2024-05-06T14:49:49.773765Z","iopub.status.idle":"2024-05-06T14:50:06.533027Z","shell.execute_reply.started":"2024-05-06T14:49:49.773737Z","shell.execute_reply":"2024-05-06T14:50:06.532053Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.2.4-py3-none-any.whl.metadata (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m749.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.2)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.2)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nDownloading lightning-2.2.4-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.2.4\n","output_type":"stream"}]},{"cell_type":"code","source":"# Basic libraries\nimport os\nimport numpy\nimport torch\nfrom torch import nn\nimport pandas\nimport matplotlib.pyplot as plt\n\n# PyTorch data processing libraries\nfrom torchvision import transforms, datasets\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# PyTorch Model Implementation Libraries\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torchmetrics import Accuracy\n\n# PyTorch Lightning\nfrom lightning import LightningDataModule, LightningModule\nimport lightning as L\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:50:11.386522Z","iopub.execute_input":"2024-05-06T14:50:11.386848Z","iopub.status.idle":"2024-05-06T14:50:21.016089Z","shell.execute_reply.started":"2024-05-06T14:50:11.386821Z","shell.execute_reply":"2024-05-06T14:50:21.014992Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Set up train and test paths\ndata_path = '/kaggle/input/agricultural-crops-image-classification/Agricultural-crops'","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:50:25.467414Z","iopub.execute_input":"2024-05-06T14:50:25.468569Z","iopub.status.idle":"2024-05-06T14:50:25.472766Z","shell.execute_reply.started":"2024-05-06T14:50:25.468528Z","shell.execute_reply":"2024-05-06T14:50:25.471820Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CropsDataModule(L.LightningDataModule):\n    def __init__(self, data_dir: str, batch_size: int = 32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.base_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n\n    def setup(self, stage: str = None):\n    # Use the ImageFolder method to arrange the dataset\n        full_dataset = datasets.ImageFolder(root=self.data_dir, transform=self.base_transform)\n\n        if stage == \"fit\" or stage is None:\n            # Split for training and validation\n            train_size = int(len(full_dataset) * 0.7)\n            val_size = int(len(full_dataset) * 0.15)\n            test_size = len(full_dataset) - train_size - val_size\n            self.train_set, self.val_set, self.test_set = random_split(\n                full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42)\n                )\n            self.train_set.dataset.transform = transforms.Compose([\n                transforms.RandomResizedCrop(224),\n                transforms.RandomHorizontalFlip(),\n                self.base_transform\n            ])\n            self.val_set.dataset.transform = transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n                self.base_transform\n            ])\n\n        if stage == \"test\" or stage == \"predict\" or stage is None:\n            # Ensure the test set uses a less aggressive transformation\n            self.test_set.dataset.transform = transforms.Compose([\n                transforms.Resize(256),\n                transforms.CenterCrop(224),\n                self.base_transform\n            ])\n\n    def train_dataloader(self):\n        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_set, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_set, batch_size=self.batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:50:27.731137Z","iopub.execute_input":"2024-05-06T14:50:27.731936Z","iopub.status.idle":"2024-05-06T14:50:27.743484Z","shell.execute_reply.started":"2024-05-06T14:50:27.731902Z","shell.execute_reply":"2024-05-06T14:50:27.742568Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Create an instance of the data class\ndm = CropsDataModule(data_dir=data_path, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:50:30.019459Z","iopub.execute_input":"2024-05-06T14:50:30.020051Z","iopub.status.idle":"2024-05-06T14:50:30.024958Z","shell.execute_reply.started":"2024-05-06T14:50:30.020022Z","shell.execute_reply":"2024-05-06T14:50:30.024084Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Setup the data module\ndm.setup()\ntrain_loader = dm.train_dataloader()\nval_loader = dm.val_dataloader()\ntest_loader = dm.test_dataloader()\n\n# Function to check a few batches\ndef check_dataloader(dataloader, name):\n    print(f\"Checking {name} DataLoader:\")\n    for i, (data, target) in enumerate(dataloader):\n        print(f\"  Batch {i + 1}:\")\n        print(f\"    Data shape: {data.shape}, Type: {data.dtype}\")\n        print(f\"    Target shape: {target.shape}, Type: {target.dtype}\")\n        if i >= 2:  # Check only the first 3 batches\n            break\n\ncheck_dataloader(train_loader, \"Training\")\ncheck_dataloader(val_loader, \"Validation\")\ncheck_dataloader(test_loader, \"Testing\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:50:56.446968Z","iopub.execute_input":"2024-05-06T14:50:56.447597Z","iopub.status.idle":"2024-05-06T14:50:59.107884Z","shell.execute_reply.started":"2024-05-06T14:50:56.447565Z","shell.execute_reply":"2024-05-06T14:50:59.106947Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Checking Training DataLoader:\n  Batch 1:\n    Data shape: torch.Size([16, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([16]), Type: torch.int64\n  Batch 2:\n    Data shape: torch.Size([16, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([16]), Type: torch.int64\n  Batch 3:\n    Data shape: torch.Size([16, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([16]), Type: torch.int64\nChecking Validation DataLoader:\n  Batch 1:\n    Data shape: torch.Size([16, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([16]), Type: torch.int64\n  Batch 2:\n    Data shape: torch.Size([16, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([16]), Type: torch.int64\n  Batch 3:\n    Data shape: torch.Size([16, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([16]), Type: torch.int64\nChecking Testing DataLoader:\n  Batch 1:\n    Data shape: torch.Size([16, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([16]), Type: torch.int64\n  Batch 2:\n    Data shape: torch.Size([16, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([16]), Type: torch.int64\n  Batch 3:\n    Data shape: torch.Size([16, 3, 224, 224]), Type: torch.float32\n    Target shape: torch.Size([16]), Type: torch.int64\n","output_type":"stream"}]},{"cell_type":"code","source":"class ImageNetTL(L.LightningModule):\n    def __init__(self, num_target_classes: int, learning_rate: float = 0.001):\n        super().__init__()\n        self.save_hyperparameters()  # This saves learning_rate and num_target_classes as part of model hyperparameters\n        self.validation_step_outputs = [] # Create an empty list to store the validation step outputs\n        self.test_step_outputs = []\n        \n        backbone = models.resnet50(weights=\"DEFAULT\")\n        num_features = backbone.fc.in_features # Save the features of the last layer\n        layers = list(backbone.children())[:-1] # Remove the last layer\n        self.feature_extractor = nn.Sequential(*layers) # Wrap the remaining layers\n        \n        self.num_target_classes = num_target_classes\n        self.classifier = nn.Linear(num_features, num_target_classes)\n        \n        # Evaluation metrics\n        self.train_acc = Accuracy(task='multiclass', num_classes=num_target_classes)\n        self.valid_acc = Accuracy(task='multiclass', num_classes=num_target_classes)\n        self.test_acc = Accuracy(task='multiclass', num_classes=num_target_classes)\n        \n    def forward(self, x):\n        representations = self.feature_extractor(x).flatten(1)\n        return self.classifier(representations)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        loss = F.cross_entropy(preds, y)\n        self.train_acc(preds, y)\n        \n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        train_acc_val = self.train_acc.compute()\n        self.log('train_acc', train_acc_val, on_step=False, on_epoch=True, prog_bar=True)\n        return {'loss': loss, 'train_acc': train_acc_val}\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        val_loss = F.cross_entropy(preds, y)\n        self.valid_acc(preds, y)\n        self.validation_step_outputs.append(val_loss)\n        self.log('val_loss', val_loss, on_step=False, on_epoch=True)\n        return {'val_loss': val_loss}\n    \n    # Added to make use of the outputs from each `validation_step`\n    def on_validation_epoch_end(self):\n        loss_average = torch.stack(self.validation_step_outputs).mean()\n        self.log(\"validation_loss_average\", loss_average)\n        self.validation_step_outputs.clear()  # free memory\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n        test_loss = F.cross_entropy(preds, y)\n        self.test_step_outputs.append(test_loss)\n        self.log(\"test_loss\", test_loss, on_step=False, on_epoch=True)\n        return {'test_loss': test_loss}\n    \n    # Added to make use of the outputs from each `test_step`\n    def on_test_epoch_end(self, outputs):\n        loss_average = torch.stack(self.test_step_outputs).mean()\n        self.log(\"test_loss_average\", loss_average)\n        self.test_step_outputs.clear()  # free memory\n        \n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:51:05.735892Z","iopub.execute_input":"2024-05-06T14:51:05.736552Z","iopub.status.idle":"2024-05-06T14:51:05.752851Z","shell.execute_reply.started":"2024-05-06T14:51:05.736522Z","shell.execute_reply":"2024-05-06T14:51:05.751999Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = ImageNetTL(num_target_classes=30)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:51:09.549139Z","iopub.execute_input":"2024-05-06T14:51:09.549941Z","iopub.status.idle":"2024-05-06T14:51:10.915754Z","shell.execute_reply.started":"2024-05-06T14:51:09.549907Z","shell.execute_reply":"2024-05-06T14:51:10.914948Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 143MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"# Run the model once quickly to check if everything is good\ntrainer = L.Trainer(fast_dev_run=True) # By default it runs (5 batches of train, validation, and test)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:51:52.027718Z","iopub.execute_input":"2024-05-06T14:51:52.028885Z","iopub.status.idle":"2024-05-06T14:51:52.820075Z","shell.execute_reply.started":"2024-05-06T14:51:52.028844Z","shell.execute_reply":"2024-05-06T14:51:52.819251Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nINFO: Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Run a sanity check in the validation loop\ntrainer = L.Trainer(num_sanity_val_steps=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:52:18.594239Z","iopub.execute_input":"2024-05-06T14:52:18.594594Z","iopub.status.idle":"2024-05-06T14:52:18.652239Z","shell.execute_reply.started":"2024-05-06T14:52:18.594565Z","shell.execute_reply":"2024-05-06T14:52:18.651415Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the model for one epoch\ndm.setup('fit')\ntrainer = L.Trainer(max_epochs=1, log_every_n_steps=10)\n#print(next(iter(dm.train_dataloader())))  # Test the iterability directly here\ntrainer.fit(model, datamodule=dm)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:53:05.207788Z","iopub.execute_input":"2024-05-06T14:53:05.208497Z","iopub.status.idle":"2024-05-06T14:53:33.598662Z","shell.execute_reply.started":"2024-05-06T14:53:05.208469Z","shell.execute_reply":"2024-05-06T14:53:33.597716Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nWARNING: Missing logger folder: /kaggle/working/lightning_logs\n2024-05-06 14:53:07.623355: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-06 14:53:07.623461: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-06 14:53:07.782725: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name              | Type               | Params\n---------------------------------------------------------\n0 | feature_extractor | Sequential         | 23.5 M\n1 | classifier        | Linear             | 61.5 K\n2 | train_acc         | MulticlassAccuracy | 0     \n3 | valid_acc         | MulticlassAccuracy | 0     \n4 | test_acc          | MulticlassAccuracy | 0     \n---------------------------------------------------------\n23.6 M    Trainable params\n0         Non-trainable params\n23.6 M    Total params\n94.278    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"901ac1ddd5c94079990d447e65264785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize EarlyStopping callback to monitor 'val_loss' for a patience of 3 epochs\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.00,\n    patience=3,\n    verbose=False,\n    mode='min'\n)\n\ntrainer = L.Trainer(max_epochs=10,\n                    profiler='simple', log_every_n_steps=10)\n# Profiler is added to check if there are bottlenecks in the code\ntrainer.fit(model, datamodule=dm)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:53:45.093731Z","iopub.execute_input":"2024-05-06T14:53:45.094446Z","iopub.status.idle":"2024-05-06T14:55:30.076358Z","shell.execute_reply.started":"2024-05-06T14:53:45.094414Z","shell.execute_reply":"2024-05-06T14:55:30.075473Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"INFO: Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nINFO: \n  | Name              | Type               | Params\n---------------------------------------------------------\n0 | feature_extractor | Sequential         | 23.5 M\n1 | classifier        | Linear             | 61.5 K\n2 | train_acc         | MulticlassAccuracy | 0     \n3 | valid_acc         | MulticlassAccuracy | 0     \n4 | test_acc          | MulticlassAccuracy | 0     \n---------------------------------------------------------\n23.6 M    Trainable params\n0         Non-trainable params\n23.6 M    Total params\n94.278    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd91ece226d4751af4a9e6017a2572b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=10` reached.\nINFO: FIT Profiler Report\n\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|  Action                                                                                                                                                         \t|  Mean duration (s)\t|  Num calls      \t|  Total time (s) \t|  Percentage %   \t|\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|  Total                                                                                                                                                          \t|  -              \t|  14867          \t|  104.91         \t|  100 %          \t|\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|  run_training_epoch                                                                                                                                             \t|  9.6972         \t|  10             \t|  96.972         \t|  92.435         \t|\n|  [_TrainingEpochLoop].train_dataloader_next                                                                                                                     \t|  0.15598        \t|  370            \t|  57.711         \t|  55.011         \t|\n|  run_training_batch                                                                                                                                             \t|  0.059353       \t|  370            \t|  21.961         \t|  20.933         \t|\n|  [LightningModule]ImageNetTL.optimizer_step                                                                                                                     \t|  0.059128       \t|  370            \t|  21.877         \t|  20.854         \t|\n|  [Strategy]SingleDeviceStrategy.training_step                                                                                                                   \t|  0.043694       \t|  370            \t|  16.167         \t|  15.41          \t|\n|  [_EvaluationLoop].val_next                                                                                                                                     \t|  0.14335        \t|  82             \t|  11.754         \t|  11.204         \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_end       \t|  0.70505        \t|  10             \t|  7.0505         \t|  6.7206         \t|\n|  [Strategy]SingleDeviceStrategy.backward                                                                                                                        \t|  0.010074       \t|  370            \t|  3.7274         \t|  3.553          \t|\n|  [Strategy]SingleDeviceStrategy.validation_step                                                                                                                 \t|  0.039314       \t|  82             \t|  3.2237         \t|  3.0729         \t|\n|  [Strategy]SingleDeviceStrategy.batch_to_device                                                                                                                 \t|  0.0035767      \t|  452            \t|  1.6167         \t|  1.541          \t|\n|  [LightningModule]ImageNetTL.transfer_batch_to_device                                                                                                           \t|  0.0034597      \t|  452            \t|  1.5638         \t|  1.4906         \t|\n|  [Callback]TQDMProgressBar.on_train_batch_end                                                                                                                   \t|  0.0012583      \t|  370            \t|  0.46559        \t|  0.4438         \t|\n|  [LightningModule]ImageNetTL.optimizer_zero_grad                                                                                                                \t|  0.00067502     \t|  370            \t|  0.24976        \t|  0.23807        \t|\n|  [Callback]TQDMProgressBar.on_validation_start                                                                                                                  \t|  0.013128       \t|  11             \t|  0.14441        \t|  0.13765        \t|\n|  [Callback]TQDMProgressBar.on_validation_batch_end                                                                                                              \t|  0.00089277     \t|  82             \t|  0.073207       \t|  0.069782       \t|\n|  [LightningModule]ImageNetTL.on_validation_model_zero_grad                                                                                                      \t|  0.0026952      \t|  10             \t|  0.026952       \t|  0.025691       \t|\n|  [LightningDataModule]CropsDataModule.setup                                                                                                                     \t|  0.021296       \t|  1              \t|  0.021296       \t|  0.0203         \t|\n|  [Callback]TQDMProgressBar.on_validation_batch_start                                                                                                            \t|  0.00020832     \t|  82             \t|  0.017083       \t|  0.016283       \t|\n|  [Callback]TQDMProgressBar.on_validation_end                                                                                                                    \t|  0.0013325      \t|  11             \t|  0.014657       \t|  0.013971       \t|\n|  [Callback]TQDMProgressBar.on_sanity_check_start                                                                                                                \t|  0.013559       \t|  1              \t|  0.013559       \t|  0.012924       \t|\n|  [Callback]TQDMProgressBar.on_train_epoch_start                                                                                                                 \t|  0.0013366      \t|  10             \t|  0.013366       \t|  0.012741       \t|\n|  [Callback]TQDMProgressBar.on_train_start                                                                                                                       \t|  0.012899       \t|  1              \t|  0.012899       \t|  0.012295       \t|\n|  [LightningModule]ImageNetTL.on_validation_model_eval                                                                                                           \t|  0.0010516      \t|  11             \t|  0.011568       \t|  0.011027       \t|\n|  [LightningModule]ImageNetTL.configure_gradient_clipping                                                                                                        \t|  3.1201e-05     \t|  370            \t|  0.011544       \t|  0.011004       \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_end       \t|  2.7823e-05     \t|  370            \t|  0.010294       \t|  0.0098127      \t|\n|  [LightningModule]ImageNetTL.on_validation_epoch_end                                                                                                            \t|  0.00086591     \t|  11             \t|  0.009525       \t|  0.0090793      \t|\n|  [Callback]ModelSummary.on_fit_start                                                                                                                            \t|  0.009367       \t|  1              \t|  0.009367       \t|  0.0089287      \t|\n|  [Callback]TQDMProgressBar.on_train_epoch_end                                                                                                                   \t|  0.00076837     \t|  10             \t|  0.0076837      \t|  0.0073242      \t|\n|  [Callback]ModelSummary.on_train_batch_end                                                                                                                      \t|  4.8078e-06     \t|  370            \t|  0.0017789      \t|  0.0016957      \t|\n|  [Callback]TQDMProgressBar.on_after_backward                                                                                                                    \t|  4.3355e-06     \t|  370            \t|  0.0016042      \t|  0.0015291      \t|\n|  [Callback]TQDMProgressBar.on_before_zero_grad                                                                                                                  \t|  4.2417e-06     \t|  370            \t|  0.0015694      \t|  0.001496       \t|\n|  [Callback]TQDMProgressBar.on_before_backward                                                                                                                   \t|  3.7261e-06     \t|  370            \t|  0.0013787      \t|  0.0013141      \t|\n|  [LightningModule]ImageNetTL.on_before_batch_transfer                                                                                                           \t|  2.8513e-06     \t|  452            \t|  0.0012888      \t|  0.0012285      \t|\n|  [LightningModule]ImageNetTL.configure_optimizers                                                                                                               \t|  0.001249       \t|  1              \t|  0.001249       \t|  0.0011906      \t|\n|  [Callback]TQDMProgressBar.on_train_batch_start                                                                                                                 \t|  3.1491e-06     \t|  370            \t|  0.0011652      \t|  0.0011106      \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_after_backward        \t|  2.7773e-06     \t|  370            \t|  0.0010276      \t|  0.00097951     \t|\n|  [Callback]TQDMProgressBar.on_before_optimizer_step                                                                                                             \t|  2.7529e-06     \t|  370            \t|  0.0010186      \t|  0.00097091     \t|\n|  [LightningModule]ImageNetTL.on_after_batch_transfer                                                                                                            \t|  2.1503e-06     \t|  452            \t|  0.00097196     \t|  0.00092648     \t|\n|  [LightningDataModule]CropsDataModule.train_dataloader                                                                                                          \t|  0.00097014     \t|  1              \t|  0.00097014     \t|  0.00092474     \t|\n|  [Callback]ModelSummary.on_before_zero_grad                                                                                                                     \t|  2.3733e-06     \t|  370            \t|  0.00087811     \t|  0.00083702     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_zero_grad      \t|  2.364e-06      \t|  370            \t|  0.0008747      \t|  0.00083377     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_batch_start     \t|  2.3605e-06     \t|  370            \t|  0.00087337     \t|  0.0008325      \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_backward       \t|  2.2392e-06     \t|  370            \t|  0.00082849     \t|  0.00078973     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_before_optimizer_step \t|  2.2271e-06     \t|  370            \t|  0.00082402     \t|  0.00078547     \t|\n|  [Callback]ModelSummary.on_before_backward                                                                                                                      \t|  2.1969e-06     \t|  370            \t|  0.00081287     \t|  0.00077483     \t|\n|  [Callback]ModelSummary.on_after_backward                                                                                                                       \t|  2.1899e-06     \t|  370            \t|  0.00081026     \t|  0.00077234     \t|\n|  [Callback]ModelSummary.on_train_batch_start                                                                                                                    \t|  2.1879e-06     \t|  370            \t|  0.00080953     \t|  0.00077165     \t|\n|  [Callback]ModelSummary.on_before_optimizer_step                                                                                                                \t|  2.1379e-06     \t|  370            \t|  0.00079102     \t|  0.00075401     \t|\n|  [Callback]TQDMProgressBar.on_train_end                                                                                                                         \t|  0.00077145     \t|  1              \t|  0.00077145     \t|  0.00073536     \t|\n|  [LightningModule]ImageNetTL.on_before_zero_grad                                                                                                                \t|  2.0382e-06     \t|  370            \t|  0.00075414     \t|  0.00071885     \t|\n|  [LightningModule]ImageNetTL.on_train_batch_start                                                                                                               \t|  2.0052e-06     \t|  370            \t|  0.00074194     \t|  0.00070722     \t|\n|  [LightningModule]ImageNetTL.on_after_backward                                                                                                                  \t|  1.9797e-06     \t|  370            \t|  0.00073251     \t|  0.00069823     \t|\n|  [Strategy]SingleDeviceStrategy.on_train_batch_start                                                                                                            \t|  1.8477e-06     \t|  370            \t|  0.00068365     \t|  0.00065166     \t|\n|  [LightningModule]ImageNetTL.on_train_batch_end                                                                                                                 \t|  1.8463e-06     \t|  370            \t|  0.00068312     \t|  0.00065115     \t|\n|  [LightningModule]ImageNetTL.on_before_backward                                                                                                                 \t|  1.7924e-06     \t|  370            \t|  0.00066317     \t|  0.00063214     \t|\n|  [LightningModule]ImageNetTL.on_before_optimizer_step                                                                                                           \t|  1.5698e-06     \t|  370            \t|  0.00058083     \t|  0.00055365     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_end        \t|  4.656e-05      \t|  11             \t|  0.00051216     \t|  0.0004882      \t|\n|  [Callback]ModelSummary.on_validation_batch_end                                                                                                                 \t|  5.6991e-06     \t|  82             \t|  0.00046732     \t|  0.00044546     \t|\n|  [LightningDataModule]CropsDataModule.val_dataloader                                                                                                            \t|  0.00042638     \t|  1              \t|  0.00042638     \t|  0.00040643     \t|\n|  [Callback]ModelSummary.on_validation_batch_start                                                                                                               \t|  3.1954e-06     \t|  82             \t|  0.00026203     \t|  0.00024977     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_start\t|  2.4804e-06     \t|  82             \t|  0.00020339     \t|  0.00019388     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_batch_end  \t|  2.4347e-06     \t|  82             \t|  0.00019964     \t|  0.0001903      \t|\n|  [LightningModule]ImageNetTL.on_validation_batch_end                                                                                                            \t|  2.0905e-06     \t|  82             \t|  0.00017142     \t|  0.0001634      \t|\n|  [LightningModule]ImageNetTL.on_validation_batch_start                                                                                                          \t|  1.8943e-06     \t|  82             \t|  0.00015533     \t|  0.00014807     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.setup                    \t|  0.00013252     \t|  1              \t|  0.00013252     \t|  0.00012632     \t|\n|  [Callback]ModelSummary.on_train_epoch_end                                                                                                                      \t|  5.1908e-06     \t|  10             \t|  5.1908e-05     \t|  4.9479e-05     \t|\n|  [Callback]ModelSummary.on_validation_start                                                                                                                     \t|  4.6682e-06     \t|  11             \t|  5.135e-05      \t|  4.8947e-05     \t|\n|  [Callback]ModelSummary.on_train_epoch_start                                                                                                                    \t|  5.1116e-06     \t|  10             \t|  5.1116e-05     \t|  4.8724e-05     \t|\n|  [Callback]ModelSummary.on_validation_end                                                                                                                       \t|  4.6429e-06     \t|  11             \t|  5.1072e-05     \t|  4.8682e-05     \t|\n|  [Callback]TQDMProgressBar.on_validation_epoch_end                                                                                                              \t|  4.357e-06      \t|  11             \t|  4.7927e-05     \t|  4.5684e-05     \t|\n|  [Callback]TQDMProgressBar.on_validation_epoch_start                                                                                                            \t|  2.7282e-06     \t|  11             \t|  3.001e-05      \t|  2.8606e-05     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_start      \t|  2.5623e-06     \t|  11             \t|  2.8185e-05     \t|  2.6866e-05     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_start\t|  2.555e-06      \t|  11             \t|  2.8105e-05     \t|  2.679e-05      \t|\n|  [Callback]ModelSummary.on_validation_epoch_end                                                                                                                 \t|  2.4224e-06     \t|  11             \t|  2.6646e-05     \t|  2.5399e-05     \t|\n|  [LightningModule]ImageNetTL.on_train_epoch_start                                                                                                               \t|  2.6625e-06     \t|  10             \t|  2.6625e-05     \t|  2.5379e-05     \t|\n|  [Callback]TQDMProgressBar.on_save_checkpoint                                                                                                                   \t|  2.6465e-06     \t|  10             \t|  2.6465e-05     \t|  2.5227e-05     \t|\n|  [Callback]ModelSummary.on_validation_epoch_start                                                                                                               \t|  2.3181e-06     \t|  11             \t|  2.5499e-05     \t|  2.4306e-05     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_validation_epoch_end  \t|  2.3164e-06     \t|  11             \t|  2.548e-05      \t|  2.4288e-05     \t|\n|  [LightningDataModule]CropsDataModule.state_dict                                                                                                                \t|  2.4792e-06     \t|  10             \t|  2.4792e-05     \t|  2.3632e-05     \t|\n|  [LightningModule]ImageNetTL.on_validation_end                                                                                                                  \t|  2.0981e-06     \t|  11             \t|  2.3079e-05     \t|  2.1999e-05     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_epoch_start     \t|  2.2681e-06     \t|  10             \t|  2.2681e-05     \t|  2.162e-05      \t|\n|  [LightningModule]ImageNetTL.on_validation_epoch_start                                                                                                          \t|  2.0478e-06     \t|  11             \t|  2.2526e-05     \t|  2.1472e-05     \t|\n|  [Strategy]SingleDeviceStrategy.on_validation_start                                                                                                             \t|  2.0328e-06     \t|  11             \t|  2.2361e-05     \t|  2.1315e-05     \t|\n|  [Strategy]SingleDeviceStrategy.on_validation_end                                                                                                               \t|  1.9962e-06     \t|  11             \t|  2.1958e-05     \t|  2.0931e-05     \t|\n|  [LightningModule]ImageNetTL.on_validation_start                                                                                                                \t|  1.9441e-06     \t|  11             \t|  2.1385e-05     \t|  2.0384e-05     \t|\n|  [LightningModule]ImageNetTL.on_train_epoch_end                                                                                                                 \t|  2.0375e-06     \t|  10             \t|  2.0375e-05     \t|  1.9422e-05     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_save_checkpoint       \t|  1.9999e-06     \t|  10             \t|  1.9999e-05     \t|  1.9063e-05     \t|\n|  [Callback]ModelSummary.on_save_checkpoint                                                                                                                      \t|  1.9493e-06     \t|  10             \t|  1.9493e-05     \t|  1.8581e-05     \t|\n|  [LightningModule]ImageNetTL.on_save_checkpoint                                                                                                                 \t|  1.3471e-06     \t|  10             \t|  1.3471e-05     \t|  1.2841e-05     \t|\n|  [Callback]TQDMProgressBar.on_sanity_check_end                                                                                                                  \t|  8.628e-06      \t|  1              \t|  8.628e-06      \t|  8.2243e-06     \t|\n|  [Callback]TQDMProgressBar.setup                                                                                                                                \t|  6.682e-06      \t|  1              \t|  6.682e-06      \t|  6.3693e-06     \t|\n|  [Callback]ModelSummary.on_train_end                                                                                                                            \t|  5.291e-06      \t|  1              \t|  5.291e-06      \t|  5.0434e-06     \t|\n|  [Callback]ModelSummary.on_sanity_check_start                                                                                                                   \t|  5.267e-06      \t|  1              \t|  5.267e-06      \t|  5.0205e-06     \t|\n|  [Callback]ModelSummary.on_train_start                                                                                                                          \t|  4.957e-06      \t|  1              \t|  4.957e-06      \t|  4.7251e-06     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_start             \t|  4.819e-06      \t|  1              \t|  4.819e-06      \t|  4.5935e-06     \t|\n|  [Callback]TQDMProgressBar.on_fit_end                                                                                                                           \t|  4.639e-06      \t|  1              \t|  4.639e-06      \t|  4.4219e-06     \t|\n|  [LightningModule]ImageNetTL.configure_callbacks                                                                                                                \t|  4.623e-06      \t|  1              \t|  4.623e-06      \t|  4.4067e-06     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_start           \t|  4.353e-06      \t|  1              \t|  4.353e-06      \t|  4.1493e-06     \t|\n|  [Callback]ModelSummary.on_sanity_check_end                                                                                                                     \t|  3.5e-06        \t|  1              \t|  3.5e-06        \t|  3.3362e-06     \t|\n|  [Callback]TQDMProgressBar.on_fit_start                                                                                                                         \t|  3.426e-06      \t|  1              \t|  3.426e-06      \t|  3.2657e-06     \t|\n|  [Callback]ModelSummary.setup                                                                                                                                   \t|  3.236e-06      \t|  1              \t|  3.236e-06      \t|  3.0846e-06     \t|\n|  [LightningDataModule]CropsDataModule.prepare_data                                                                                                              \t|  3.161e-06      \t|  1              \t|  3.161e-06      \t|  3.0131e-06     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_start    \t|  3.046e-06      \t|  1              \t|  3.046e-06      \t|  2.9035e-06     \t|\n|  [Callback]TQDMProgressBar.teardown                                                                                                                             \t|  2.86e-06       \t|  1              \t|  2.86e-06       \t|  2.7262e-06     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_sanity_check_end      \t|  2.791e-06      \t|  1              \t|  2.791e-06      \t|  2.6604e-06     \t|\n|  [LightningModule]ImageNetTL.setup                                                                                                                              \t|  2.377e-06      \t|  1              \t|  2.377e-06      \t|  2.2658e-06     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_train_end             \t|  2.371e-06      \t|  1              \t|  2.371e-06      \t|  2.2601e-06     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.teardown                 \t|  2.233e-06      \t|  1              \t|  2.233e-06      \t|  2.1285e-06     \t|\n|  [Callback]ModelSummary.teardown                                                                                                                                \t|  2.23e-06       \t|  1              \t|  2.23e-06       \t|  2.1257e-06     \t|\n|  [LightningDataModule]CropsDataModule.teardown                                                                                                                  \t|  2.218e-06      \t|  1              \t|  2.218e-06      \t|  2.1142e-06     \t|\n|  [Strategy]SingleDeviceStrategy.on_train_start                                                                                                                  \t|  2.189e-06      \t|  1              \t|  2.189e-06      \t|  2.0866e-06     \t|\n|  [LightningModule]ImageNetTL.on_train_start                                                                                                                     \t|  2.187e-06      \t|  1              \t|  2.187e-06      \t|  2.0847e-06     \t|\n|  [LightningModule]ImageNetTL.on_fit_end                                                                                                                         \t|  2.15e-06       \t|  1              \t|  2.15e-06       \t|  2.0494e-06     \t|\n|  [Callback]ModelSummary.on_fit_end                                                                                                                              \t|  2.09e-06       \t|  1              \t|  2.09e-06       \t|  1.9922e-06     \t|\n|  [Callback]ModelCheckpoint{'monitor': None, 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}.on_fit_end               \t|  2.074e-06      \t|  1              \t|  2.074e-06      \t|  1.977e-06      \t|\n|  [LightningModule]ImageNetTL.on_train_end                                                                                                                       \t|  2.031e-06      \t|  1              \t|  2.031e-06      \t|  1.936e-06      \t|\n|  [Strategy]SingleDeviceStrategy.on_train_end                                                                                                                    \t|  1.987e-06      \t|  1              \t|  1.987e-06      \t|  1.894e-06      \t|\n|  [LightningModule]ImageNetTL.on_fit_start                                                                                                                       \t|  1.737e-06      \t|  1              \t|  1.737e-06      \t|  1.6557e-06     \t|\n|  [LightningModule]ImageNetTL.prepare_data                                                                                                                       \t|  1.674e-06      \t|  1              \t|  1.674e-06      \t|  1.5957e-06     \t|\n|  [LightningModule]ImageNetTL.teardown                                                                                                                           \t|  1.614e-06      \t|  1              \t|  1.614e-06      \t|  1.5385e-06     \t|\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Launch TensorBoard to display outputs\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:56:50.192808Z","iopub.execute_input":"2024-05-06T14:56:50.193201Z","iopub.status.idle":"2024-05-06T14:56:50.206174Z","shell.execute_reply.started":"2024-05-06T14:56:50.193169Z","shell.execute_reply":"2024-05-06T14:56:50.205335Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Reusing TensorBoard on port 6006 (pid 149), started 0:00:58 ago. (Use '!kill 149' to kill it.)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-9262b88b8e23d620\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-9262b88b8e23d620\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}